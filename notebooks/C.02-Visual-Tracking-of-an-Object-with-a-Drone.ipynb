{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "<!--NOTEBOOK_HEADER-->\n",
    "*This notebook contains course material from [CBE 30338 Chemical Process Control](http://jckantor.github.io/CBE30338/) \n",
    "by Jeffrey Kantor (jeff at nd.edu); the content is available [on GitHub](https://github.com/jckantor/CBE30338/).\n",
    "The text is released under the [CC-BY-NC-ND-4.0 license](https://creativecommons.org/licenses/by-nc-nd/4.0/legalcode), \n",
    "and code is released under the [MIT license](https://opensource.org/licenses/MIT).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "<!--NAVIGATION-->\n",
    "< [Diabetes: Controlling Blood Glucose Concentrations](http://nbviewer.jupyter.org/github/jckantor/CBE30338/blob/master/notebooks/C.01-Diabetes-Controlling-Blood-Glucose-Concentrations.ipynb) | [Contents](toc.ipynb) |<p><a href=\"https://colab.research.google.com/github/jckantor/CBE30338/blob/master/notebooks/C.02-Visual-Tracking-of-an-Object-with-a-Drone.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open in Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9sw-WwInN3qQ",
    "pycharm": {}
   },
   "source": [
    "# Visual Tracking of an Object with a Drone\n",
    "\n",
    "CBE 30338 Chemical Process Control, Spring 2019\n",
    "\n",
    "Elizabeth Innis, Kari Minnich, Omosefe Obanor, Alyssa Schuettpelz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mPc3jFu5Td49",
    "pycharm": {}
   },
   "source": [
    "## Problem Statement\n",
    "\n",
    "The object recognition and tracking of a specific object using a DJI Tello requires a method to communicate changes in the object's position as well as manipulation of the RBG values in a CV2 code. For object recognition, the DJI Tello will need to recognize the specific color of the object. The RBG values chosen for the object are unlikely to be exact, but it will need a specific range to be close enough in order for the Tello to recognize the specific object. To track the object, the DJI Tello will need to process that there is an offset in the original distance of the object from the center of frame and react accordingly to refocus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mPc3jFu5Td49",
    "pycharm": {}
   },
   "source": [
    "### Technology Challenge\n",
    "\n",
    "This project uses software and coding to operate a DJI Tello Drone. Working with hardware presents a set of challenges that modeling with only software or running simulations does not provide. The issues of dying batteries and overheating had to be overcome before the code could be tested to run commands. It should be noted that if the battery life of the Tello is too low, the drone will not take flight. Before running code, charge all of the battery packs and have replacements ready if the drone will not lift off. In addition to this, there was a point during exploration that the batteries were known to be fully charged, yet the drone would not lift from the ground. The DJI Tello will not take flight if it senses that it is overheating. Carpeted floors have been observed to prevent the DJI Tello from cooling down. This can be circumvented by taking off from solid surfaces or placing a notebook, folder or other convenient smooth, flat object under the drone as a launch surface. Finally, several libraries must be downloaded to run the video feed of the Tello with CV2. Those libraries can be found in the appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mPc3jFu5Td49",
    "pycharm": {}
   },
   "source": [
    "### Project Goal \n",
    "\n",
    "The goal of this project is to manipulate the operation of a working drone to enable recognition and tracking. The target detection will be optimized to incorporate internal camera streaming, an analysis of the stream, and subsequent reaction. Existing code repositories are utilized as a reference at a foundational level to guide the target acquisition method. Several applications exist in which such process control for target detection is beneficial. Key areas of use are in search and rescue, as well as other humanitarian efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tn6qyMasOCfu",
    "pycharm": {}
   },
   "source": [
    "## Results and Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MoT3WA3NNBFV",
    "pycharm": {}
   },
   "source": [
    "Operating the DJI Tello with keyboard commands is an important first step to completing the goal for the tracking of an object. The keyboard commands help the user to place the drone where he or she prefers, (proper height, angle, distance from object, etc.) in a ready position for tracking. \n",
    "\n",
    "Once the DJI Tello can be controlled remotely, the next step to color tracking is to display the camera feed that the Tello sees on the computer using CV2. The pixels in the image can be searched for pixels of a certain color by analyzing RGB values. In this project we used a range of blue values. The code will blur together pixels within the largest area blue color range and draw a circle around the blue object we hope to track. \n",
    "\n",
    "Tracking the object is done using a PID control loop incorporating the offset coordinates of the center of the object and the center of the frame. This is done by using the coordinates from the previous frame and comparing it to the current frame. These coordinate distances are computed in pixels. The goal of tracking is to track where the blue object is \"now\" based off of where it was in the last frame. \n",
    "\n",
    "The control loop uses the offset coordinates and the distance from the center to find velocities. These velocities are used to command the drone to move left, right, up, or down. It runs the loop for each frame and moves the drone accordingly. The drone should move either up, down, left, or right by observing the sign of the velocity in either the x or y direction.  The velocity at which the drone moves is the magnitude of the value computed by the control loop. All of this should result in a drone tracking a blue object. \n",
    "\n",
    "More details about each step of or project are found below. For the full commented code see the appendix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pw4vTYpSQ6Ik",
    "pycharm": {}
   },
   "source": [
    "### Controller\n",
    "  This code utilizes key presses to control the movements of the drone when tracking is not enabled. These are described by the following code: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ML1vVL4SSEY-",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    " def init_controls(self):\n",
    "        \"\"\"Define keys and add listener\"\"\"\n",
    "        self.controls = {\n",
    "            'w': 'forward',\n",
    "            's': 'backward',\n",
    "            'a': 'left',\n",
    "            'd': 'right',\n",
    "            'Key.space': 'up',\n",
    "            'Key.shift': 'down',\n",
    "            'Key.shift_r': 'down',\n",
    "            'q': 'counter_clockwise',\n",
    "            'e': 'clockwise',\n",
    "            'i': lambda speed: self.drone.flip_forward(),\n",
    "            'k': lambda speed: self.drone.flip_back(),\n",
    "            'j': lambda speed: self.drone.flip_left(),\n",
    "            'l': lambda speed: self.drone.flip_right(),\n",
    "            # arrow keys for fast turns and altitude adjustments\n",
    "            'Key.left': lambda speed: self.drone.counter_clockwise(speed),\n",
    "            'Key.right': lambda speed: self.drone.clockwise(speed),\n",
    "            'Key.up': lambda speed: self.drone.up(speed),\n",
    "            'Key.down': lambda speed: self.drone.down(speed),\n",
    "            'Key.tab': lambda speed: self.drone.takeoff(),\n",
    "            'Key.backspace': lambda speed: self.drone.land(),\n",
    "            'p': lambda speed: self.palm_land(speed),\n",
    "            't': lambda speed: self.toggle_tracking(speed),\n",
    "            'r': lambda speed: self.toggle_recording(speed),\n",
    "            'z': lambda speed: self.toggle_zoom(speed),\n",
    "            'Key.enter': lambda speed: self.take_picture(speed)\n",
    "        }\n",
    "        self.key_listener = keyboard.Listener(on_press=self.on_press,\n",
    "                                              on_release=self.on_release)\n",
    "        self.key_listener.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MlFtNerTTJt0",
    "pycharm": {}
   },
   "source": [
    "### Image Recognition\n",
    "\n",
    "This code conducts image recogniton using CV2 and the video feed from the Tello camera. A range of color values is determined for the desired object and the video stream is converted, frame by frame, into an array of RGB values. The location of the object is determined by analyzing the RBG values and looking for values within the defined range. A circle is drawn surrounding object within the RBG range, and the drone attempts to maintain that circle around the object as it adjusts in flight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJC3dxpaWhWz",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# This code does object recognition using CV2 and the video feed from the Tello camera, \n",
    "\n",
    "# A range of color values is determined for the desired object\n",
    "color_lower = (110, 50, 50)  #BGR\n",
    "color_upper = (130, 255, 255) #RGB\n",
    "\n",
    "#The video feed from the drone is then convereted, frame by frame, into an array of RGB values.\n",
    "frame = get_frame(vid_stream, stream)\n",
    "height, width = frame.shape[0], frame.shape[1]\n",
    "colortracker = Tracker(height, width, color_lower, color_upper)\n",
    "def get_frame(vid_stream, stream):\n",
    "\t\"\"\"grab the current video frame\"\"\"\n",
    "\tframe = vid_stream.read()\n",
    "\t# handle the frame from VideoCapture or VideoStream\n",
    "\tframe = frame[1] if stream else frame\n",
    "\t# if we are viewing a video and we did not grab a frame,\n",
    "\t# then we have reached the end of the video\n",
    "\tif frame is None:\n",
    "\t\treturn None\n",
    "\telse:\n",
    "\t\tframe = imutils.resize(frame, width=600)\n",
    "\t\treturn frame\n",
    "def track(self, frame):\n",
    "\t\"\"\"Simple HSV color space tracking\"\"\"\n",
    "\t# resize the frame, blur it, and convert it to the HSV\n",
    "\t# color space\n",
    "\tblurred = cv2.GaussianBlur(frame, (11, 11), 0)\n",
    "\thsv = cv2.cvtColor(blurred, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "\t# construct a mask for the color then perform\n",
    "\t# a series of dilations and erosions to remove any small\n",
    "\t# blobs left in the mask\n",
    "\tmask = cv2.inRange(hsv, self.color_lower, self.color_upper)\n",
    "\tmask = cv2.erode(mask, None, iterations=2)\n",
    "\tmask = cv2.dilate(mask, None, iterations=2)\n",
    "\n",
    "\t# find contours in the mask and initialize the current\n",
    "\t# (x, y) center of the ball\n",
    "\tcnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL,\n",
    "\t\t\t\t\t\t\tcv2.CHAIN_APPROX_SIMPLE)\n",
    "\tcnts = cnts[0]\n",
    "\tcenter = None\n",
    "\n",
    "\t# only proceed if at least one contour was found\n",
    "\tif len(cnts) > 0:\n",
    "\t\t# find the largest contour in the mask, then use\n",
    "\t\t# it to compute the minimum enclosing circle and\n",
    "\t\t# centroid\n",
    "# Once the center of the object is determined, its distance from the center of the frame is calculated. \n",
    "\n",
    "for c in cnts: #iterate through every contour\n",
    "\t\t\t\t#c = max(cnts, key=cv2.contourArea)\n",
    "\t\t\t\t((x, y), radius) = cv2.minEnclosingCircle(c)\n",
    "\t\t\t\tM = cv2.moments(c)\n",
    "\t\t\t\tcenter = (int(M[\"m10\"] / M[\"m00\"]), int(M[\"m01\"] / M[\"m00\"]))\n",
    "\n",
    "\t\t\t\t# only proceed if the radius meets a minimum size\n",
    "\t\t\t\tif radius > 10:\n",
    "\t\t\t\t\t\n",
    "\n",
    "\t\t\t\t\ttemp_xoffset = int(center[0] - self.midx) #store the xoffset and yoffset for each iteration of the loop\n",
    "\t\t\t\t\ttemp_yoffset = int(self.midy - center[1])\n",
    "\n",
    "\t\t\t\t\t#calculate the distance from the previous x and y by finding the squared error\n",
    "\t\t\t\t\tsqrd_error = ((temp_xoffset-self.xoffset)**2) + ((temp_yoffset - self.yoffset)**2)\n",
    "\n",
    "\t\t\t\t\t#Set xoffset and yoffset for the lowest possible distance\n",
    "\t\t\t\t\tif lowest_error is None or sqrd_error < lowest_error:\n",
    "\t\t\t\t\t\tlowest_error = sqrd_error\n",
    "\t\t\t\t\t\tbest_xoffset = temp_xoffset\n",
    "\t\t\t\t\t\tbest_yoffset = temp_yoffset\n",
    "\t\t\t\t\t\tbest_x = x\n",
    "\t\t\t\t\t\tbest_y = y\n",
    "\t\t\t\t\t\tbest_radius = radius\n",
    "\t\t\t\t\t\tbest_center = center\n",
    "\n",
    "\n",
    "\t\t\t#Set xoffset and yoffset to the best values calculated in the for loop\t\t\n",
    "\t\t\tself.xoffset = best_xoffset\n",
    "\t\t\tself.yoffset = best_yoffset\n",
    "\t\t\t# draw the circle and centroid on the frame,\n",
    "\t\t\t# then update the list of tracked points\n",
    "\t\t\tcv2.circle(frame, (int(best_x), int(best_y)), int(best_radius), # Draws the yellow circle on video stream\n",
    "\t\t\t\t\t(0, 255, 255), 2)\n",
    "\t\t\tcv2.circle(frame, best_center, 5, (0, 0, 255), -1) # Draws a red dot in the center of the yellow circle\n",
    "\t\telse:\n",
    "\t\t\tself.xoffset = 0\n",
    "\t\t\tself.yoffset = 0\n",
    "\n",
    "\t\treturn self.xoffset, self.yoffset #feed the optimized xoffset and yoffset to telloCV.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x94ItQPgZqCD",
    "pycharm": {}
   },
   "source": [
    "### PID Control for Object Tracking\n",
    "\n",
    " We wanted to use a feedback control loop to direct the drone towards the recognized object from tracker.py\n",
    "  to do this we will used a PID controller.   \n",
    "  This code finds an optimal x and y velocity (pixles/second) then tells the drone to go right, left, up, or down,\n",
    "  at the designated velocity. \n",
    "  The governing equations for the PID loop were found and adjusted from the course notes linked below. \n",
    " https://nbviewer.jupyter.org/github/jckantor/CBE30338/blob/master/notebooks/04.01-Implementing_PID_Control_with_Python_Yield_Statement.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "214xQsZ-aLVD",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "  \n",
    "    def init_PID(self):\n",
    "        def proportional():\n",
    "            Vx = 0\n",
    "            Vy = 0\n",
    "            prev_time = time.time()\n",
    "            Ix = 0\n",
    "            Iy = 0\n",
    "            ex_prev = 0\n",
    "            ey_prev = 0\n",
    "            while True:\n",
    "                #yield an x and y velocity from xoff, yoff, and distance\n",
    "                xoff, yoff, distance = yield Vx, Vy\n",
    "\n",
    "                #PID Calculations\n",
    "                ex = xoff - distance\n",
    "                ey = yoff - distance\n",
    "                current_time = time.time()\n",
    "                delta_t = current_time - prev_time\n",
    "                \n",
    "                #Control Equations, constants are adjusted as needed\n",
    "                Px = 0.1*ex\n",
    "                Py = 0.1*ey\n",
    "                Ix = Ix + -0.001*ex*delta_t\n",
    "                Iy = Iy + -0.001*ey*delta_t\n",
    "                Dx = 0.01*(ex - ex_prev)/(delta_t)\n",
    "                Dy = 0.01*(ey - ey_prev)/(delta_t)\n",
    "\n",
    "                Vx = Px + Ix + Dx\n",
    "                Vy = Py + Iy + Dy\n",
    "\n",
    "                #update the stored data for the next iteration\n",
    "                ex_prev = ex\n",
    "                ey_prev = ey\n",
    "                prev_time = current_time\n",
    "        self.PID = proportional()\n",
    "        self.PID.send(None)    \n",
    "\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"convert frame to cv2 image and show\"\"\"\n",
    "        image = cv2.cvtColor(numpy.array(\n",
    "            frame.to_image()), cv2.COLOR_RGB2BGR)\n",
    "        image = self.write_hud(image)\n",
    "        if self.record:\n",
    "            self.record_vid(frame)\n",
    "        distance = 0\n",
    "        xoff, yoff = self.colortracker.track(image)\n",
    "        image = self.colortracker.draw_arrows(image)\n",
    "        Vx,Vy=self.PID.send([xoff, yoff, distance])        \n",
    "        # print(\"TARGET_V: ({Vx},{Vy})\".format(Vx=Vx,Vy=Vy)) # Print statement to ensure Vx and Vy are reasonable values (<50)   \n",
    "                \n",
    "        # Create a loop to implement the Vx and Vy as a command to move the drone accordingly\n",
    "        cmd = \"\" \n",
    "        speed = 0\n",
    "\n",
    "        if self.tracking:\n",
    "            if abs(Vx) > abs(Vy):\n",
    "                if Vx > 0:\n",
    "                    cmd = \"right\"\n",
    "                    speed = abs(Vx)\n",
    "                elif Vx < 0:\n",
    "                    cmd = \"left\"\n",
    "                    speed = abs(Vx)\n",
    "            else:\n",
    "                if Vy > 0:\n",
    "                    cmd = \"up\"\n",
    "                    speed = abs(Vy)\n",
    "                elif Vy < 0:\n",
    "                    cmd = \"down\"\n",
    "                    speed = abs(Vy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1orp3iFRONME",
    "pycharm": {}
   },
   "source": [
    "## Executable Element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7abyYzIj0c-2",
    "pycharm": {}
   },
   "source": [
    "https://github.com/einnis01/CBE30338_Drone_Project/blob/master/DroneSmaller.mp4 \n",
    "\n",
    "The link above contains a video of the DJI Tello drone tracking a blue ball."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7hAHY2TpOb2X",
    "pycharm": {}
   },
   "source": [
    "## Conclusion and Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4VBlCB3GmojF",
    "pycharm": {}
   },
   "source": [
    "Our goal for this project was to enable object recognition and tracking. We were able to accomplish our goal with the implementation of a PID controller and adjusting the RGB values in the CV2 code for our specific object's color. \n",
    "\n",
    "The RGB values we chose were lower bounds: [90,50,50] and upper bounds: [110,255,255]. These values gave us the best range for the specific blue of our object. The DJI Tello was able to recognize our object very well, but it is important to note that the object could not be in front of a black background or in extremely bright yellow lights. Through various trials, it was evident that a black background and/or bright yellow lights weakened the Tello's ability to recognize the object due to the color feedback the Tello was receiving, which was no longer in the specified range.\n",
    "\n",
    "We successfully tracked our object using the object tracker in the CV2 code and the PID controller. Moving the object slowly away from the drone allowed the object tracker and PID controller time to process the changes and provide feeback to the drone. There were slight issues with oscillations when the object was moving too fast away from the center of frame. Therefore, slight adjustments were made to the constants in the PID controller to improve the stability of the drone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_q3nWI8eOimj",
    "pycharm": {}
   },
   "source": [
    "## Appendices\n",
    "All code can be found at https://github.com/einnis01/CBE30338_Drone_Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yMPXQbjNcm5_",
    "pycharm": {}
   },
   "source": [
    "### A.) TelloCV.py\n",
    "This code is the main controller for drone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bXKUEZm0OhBZ",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tellotracker:\n",
    "Allows manual operation of the drone and demo tracking mode.\n",
    "\n",
    "Requires mplayer to record/save video.\n",
    "\n",
    "Controls:\n",
    "- tab to lift off\n",
    "- WASD to move the drone\n",
    "- space/shift to ascend/descent slowly\n",
    "- Q/E to yaw slowly\n",
    "- arrow keys to ascend, descend, or yaw quickl\n",
    "- backspace to land, or P to palm-land\n",
    "- enter to take a picture\n",
    "- R to start recording video, R again to stop recording\n",
    "  (video and photos will be saved to a timestamped file in ~/Pictures/)\n",
    "- Z to toggle camera zoom state\n",
    "  (zoomed-in widescreen or high FOV 4:3)\n",
    "- T to toggle tracking  \n",
    "@author Leonie Buckley, Saksham Sinha and Jonathan Byrne\n",
    "@copyright 2018 see license file for details\n",
    "\"\"\"\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import tellopy\n",
    "import numpy\n",
    "import av   \n",
    "import cv2.cv2 as cv2\n",
    "from pynput import keyboard\n",
    "from tracker import Tracker\n",
    "import threading #Python library that allows you to create multiple threads to run multiple functions at the same time. \n",
    "\n",
    "current_frame = None # initialize global variable current_frame as None until it is changed in frame_save #numpy.array([[[0,0,0]]])  #initialize global variable as a single black pixel\n",
    "exiting = False\n",
    "\n",
    "def frame_save(tellotrack):\n",
    "    \"\"\" Stores frames from the Tello VideoStream to a temporary variable \"\"\"\n",
    "    global current_frame # allows the global variable defined above to be changed inside this function\n",
    "    global exiting\n",
    "    for packet in tellotrack.container.demux((tellotrack.vid_stream,)):\n",
    "        try:\n",
    "            for frame in packet.decode():\n",
    "                current_frame = frame\n",
    "        except Exception as e:\n",
    "            print(\"ERROR!!! {}\".format(e))\n",
    "    exiting = True\n",
    "    print(\"EXITING FRAME_SAVE THREAD!\")\n",
    "            \n",
    "def main():\n",
    "    \"\"\" Create a tello controller and show the video feed.\"\"\"\n",
    "    global current_frame # allows the global variable defined above to be changed inside this function\n",
    "    global exiting\n",
    "    tellotrack = TelloCV()\n",
    "    threading.Thread(target=frame_save, args=[tellotrack]).start() #Create a thread that runs the function frame_save while main runs in the current thread\n",
    "    \n",
    "    while not exiting:\n",
    "        if not current_frame is None: #don't run until there is a new current_frame\n",
    "            image = tellotrack.process_frame(current_frame)\n",
    "            cv2.imshow('tello', image)\n",
    "            _ = cv2.waitKey(1) & 0xFF\n",
    "    print(\"EXITING MAIN THREAD!\")\n",
    "\n",
    "class TelloCV(object):\n",
    "    \"\"\"\n",
    "    TelloTracker builds keyboard controls on top of TelloPy as well\n",
    "    as generating images from the video stream and enabling opencv support\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.prev_flight_data = None\n",
    "        self.record = False\n",
    "        self.tracking = False\n",
    "        self.keydown = False\n",
    "        self.date_fmt = '%Y-%m-%d_%H%M%S'\n",
    "        self.speed = 50\n",
    "        self.drone = tellopy.Tello()\n",
    "        self.init_drone()\n",
    "        self.init_controls()\n",
    "\n",
    "        # container for processing the packets into frames\n",
    "        self.container = av.open(self.drone.get_video_stream())\n",
    "        self.vid_stream = self.container.streams.video[0]\n",
    "        self.out_file = None\n",
    "        self.out_stream = None\n",
    "        self.out_stream_writer = None\n",
    "        self.out_name = None\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        # tracking a color  \n",
    "        #Assign a range of RGB values for the drone to look for\n",
    "        #green_lower = (30, 50, 50) \n",
    "        #green_upper = (80, 255, 255)\n",
    "        #red_lower = (0, 50, 50)\n",
    "        #red_upper = (20, 255, 255)\n",
    "        #blue_lower = (0, 0, 130)\n",
    "        #upper_blue = (150, 220, 255)\n",
    "        color_lower = (90, 50, 50)\n",
    "        color_upper =(110, 255, 255,)\n",
    "        self.track_cmd = \"\"\n",
    "        self.track_speed = 0\n",
    "        self.init_PID()\n",
    "        self.colortracker = Tracker(self.vid_stream.height,\n",
    "                               self.vid_stream.width,\n",
    "                               color_lower, color_upper)\n",
    "\n",
    "    def init_drone(self):\n",
    "        \"\"\"Connect, uneable streaming and subscribe to events\"\"\"\n",
    "        # self.drone.log.set_level(2)\n",
    "        self.drone.connect()\n",
    "        self.drone.start_video()\n",
    "        self.drone.subscribe(self.drone.EVENT_FLIGHT_DATA,\n",
    "                             self.flight_data_handler)\n",
    "        self.drone.subscribe(self.drone.EVENT_FILE_RECEIVED,\n",
    "                             self.handle_flight_received)\n",
    "\n",
    "\n",
    "    def on_press(self, keyname):\n",
    "        \"\"\"handler for keyboard listener\"\"\"\n",
    "        if self.keydown:\n",
    "            return\n",
    "        try:\n",
    "            self.keydown = True\n",
    "            keyname = str(keyname).strip('\\'')\n",
    "            print('+' + keyname)\n",
    "            if keyname == 'Key.esc':\n",
    "                self.drone.quit()\n",
    "                exit(0)\n",
    "            if keyname in self.controls:\n",
    "                key_handler = self.controls[keyname]\n",
    "                if isinstance(key_handler, str):\n",
    "                    getattr(self.drone, key_handler)(self.speed)\n",
    "                else:\n",
    "                    key_handler(self.speed)\n",
    "        except AttributeError:\n",
    "            print('special key {0} pressed'.format(keyname))\n",
    "\n",
    "    def on_release(self, keyname):\n",
    "        \"\"\"Reset on key up from keyboard listener\"\"\"\n",
    "        self.keydown = False\n",
    "        keyname = str(keyname).strip('\\'')\n",
    "        print('-' + keyname)\n",
    "        if keyname in self.controls:\n",
    "            key_handler = self.controls[keyname]\n",
    "            if isinstance(key_handler, str):\n",
    "                getattr(self.drone, key_handler)(0)\n",
    "            else:\n",
    "                key_handler(0)\n",
    "\n",
    "    def init_controls(self):\n",
    "        \"\"\"Define keys and add listener\"\"\"\n",
    "        self.controls = {\n",
    "            'w': 'forward',\n",
    "            's': 'backward',\n",
    "            'a': 'left',\n",
    "            'd': 'right',\n",
    "            'Key.space': 'up',\n",
    "            'Key.shift': 'down',\n",
    "            'Key.shift_r': 'down',\n",
    "            'q': 'counter_clockwise',\n",
    "            'e': 'clockwise',\n",
    "            'i': lambda speed: self.drone.flip_forward(),\n",
    "            'k': lambda speed: self.drone.flip_back(),\n",
    "            'j': lambda speed: self.drone.flip_left(),\n",
    "            'l': lambda speed: self.drone.flip_right(),\n",
    "            # arrow keys for fast turns and altitude adjustments\n",
    "            'Key.left': lambda speed: self.drone.counter_clockwise(speed),\n",
    "            'Key.right': lambda speed: self.drone.clockwise(speed),\n",
    "            'Key.up': lambda speed: self.drone.up(speed),\n",
    "            'Key.down': lambda speed: self.drone.down(speed),\n",
    "            'Key.tab': lambda speed: self.drone.takeoff(),\n",
    "            'Key.backspace': lambda speed: self.drone.land(),\n",
    "            'p': lambda speed: self.palm_land(speed),\n",
    "            't': lambda speed: self.toggle_tracking(speed),\n",
    "            'r': lambda speed: self.toggle_recording(speed),\n",
    "            'z': lambda speed: self.toggle_zoom(speed),\n",
    "            'Key.enter': lambda speed: self.take_picture(speed)\n",
    "        }\n",
    "        self.key_listener = keyboard.Listener(on_press=self.on_press,\n",
    "                                              on_release=self.on_release)\n",
    "        self.key_listener.start()\n",
    "        # self.key_listener.join()\n",
    "\n",
    "    #We want to use a feedback control loop to direct the drone towards the recognized object from tracker.py\n",
    "    #to do this we will use a PID controller.   \n",
    "        #find an optimal Vx and Vy using a PID\n",
    "        # distance = 100 # setpoint, pixels\n",
    "        # Vx = 0      #manipulated variable\n",
    "        # Vy = 0      #manipulated variable \n",
    "    def init_PID(self):\n",
    "        def proportional():\n",
    "            Vx = 0\n",
    "            Vy = 0\n",
    "            prev_time = time.time()\n",
    "            Ix = 0\n",
    "            Iy = 0\n",
    "            ex_prev = 0\n",
    "            ey_prev = 0\n",
    "            while True:\n",
    "                #yield an x and y velocity from xoff, yoff, and distance\n",
    "                xoff, yoff, distance = yield Vx, Vy\n",
    "\n",
    "                #PID Calculations\n",
    "                ex = xoff - distance\n",
    "                ey = yoff - distance\n",
    "                current_time = time.time()\n",
    "                delta_t = current_time - prev_time\n",
    "                \n",
    "                #Control Equations, constants are adjusted as needed\n",
    "                Px = 0.1*ex\n",
    "                Py = 0.1*ey\n",
    "                Ix = Ix + -0.001*ex*delta_t\n",
    "                Iy = Iy + -0.001*ey*delta_t\n",
    "                Dx = 0.01*(ex - ex_prev)/(delta_t)\n",
    "                Dy = 0.01*(ey - ey_prev)/(delta_t)\n",
    "\n",
    "                Vx = Px + Ix + Dx\n",
    "                Vy = Py + Iy + Dy\n",
    "\n",
    "                #update the stored data for the next iteration\n",
    "                ex_prev = ex\n",
    "                ey_prev = ey\n",
    "                prev_time = current_time\n",
    "        self.PID = proportional()\n",
    "        self.PID.send(None)    \n",
    "\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"convert frame to cv2 image and show\"\"\"\n",
    "        image = cv2.cvtColor(numpy.array(\n",
    "            frame.to_image()), cv2.COLOR_RGB2BGR)\n",
    "        image = self.write_hud(image)\n",
    "        if self.record:\n",
    "            self.record_vid(frame)\n",
    "        distance = 0\n",
    "        xoff, yoff = self.colortracker.track(image)\n",
    "        image = self.colortracker.draw_arrows(image)\n",
    "        Vx,Vy=self.PID.send([xoff, yoff, distance])        \n",
    "        # print(\"TARGET_V: ({Vx},{Vy})\".format(Vx=Vx,Vy=Vy)) # Print statement to ensure Vx and Vy are reasonable values (<50)   \n",
    "                \n",
    "        # Create a loop to implement the Vx and Vy as a command to move the drone accordingly\n",
    "        cmd = \"\" \n",
    "        speed = 0\n",
    "\n",
    "        if self.tracking:\n",
    "            if abs(Vx) > abs(Vy):\n",
    "                if Vx > 0:\n",
    "                    cmd = \"right\"\n",
    "                    speed = abs(Vx)\n",
    "                elif Vx < 0:\n",
    "                    cmd = \"left\"\n",
    "                    speed = abs(Vx)\n",
    "            else:\n",
    "                if Vy > 0:\n",
    "                    cmd = \"up\"\n",
    "                    speed = abs(Vy)\n",
    "                elif Vy < 0:\n",
    "                    cmd = \"down\"\n",
    "                    speed = abs(Vy)\n",
    "            \n",
    "        #Original Code:\n",
    "        # if self.tracking:\n",
    "        #     if xoff < -distance:\n",
    "        #         cmd = \"counter_clockwise\"\n",
    "        #     elif xoff > distance:\n",
    "        #         cmd = \"clockwise\"\n",
    "        #     elif yoff < -distance:\n",
    "        #         cmd = \"down\"\n",
    "        #     elif yoff > distance:\n",
    "        #         cmd = \"up\"\n",
    "        #     else:\n",
    "        #         if self.track_cmd is not \"\":\n",
    "        #             getattr(self.drone, self.track_cmd)(0)\n",
    "        #             self.track_cmd = \"\"\n",
    "\n",
    "        print(self.track_cmd)\n",
    "        if cmd is not self.track_cmd or speed != self.track_speed: #!= means not equal to\n",
    "            if cmd is not \"\":\n",
    "                print(\"track command:\", cmd)\n",
    "                print(\"track speed:\", speed)\n",
    "                getattr(self.drone, cmd)(speed)\n",
    "                self.track_cmd = cmd\n",
    "                self.track_speed = speed\n",
    "            else:\n",
    "                if self.track_cmd is not \"\":\n",
    "                    getattr(self.drone, self.track_cmd)(0)\n",
    "                    self.track_cmd = \"\"\n",
    "                    print(\"STOPPING!!!!!!!!\")\n",
    "                    getattr(self.drone, \"down\")(0) #Stop the drone\n",
    "                    getattr(self.drone, \"right\")(0) #Stop the drone\n",
    "        return image\n",
    "\n",
    "    def write_hud(self, frame):\n",
    "        \"\"\"Draw drone info, tracking and record on frame\"\"\"\n",
    "        stats = self.prev_flight_data.split('|')\n",
    "        stats.append(\"Tracking:\" + str(self.tracking))\n",
    "        if self.drone.zoom:\n",
    "            stats.append(\"VID\")\n",
    "        else:\n",
    "            stats.append(\"PIC\")\n",
    "        if self.record:\n",
    "            diff = int(time.time() - self.start_time)\n",
    "            mins, secs = divmod(diff, 60)\n",
    "            stats.append(\"REC {:02d}:{:02d}\".format(mins, secs))\n",
    "\n",
    "        for idx, stat in enumerate(stats):\n",
    "            text = stat.lstrip()\n",
    "            cv2.putText(frame, text, (0, 30 + (idx * 30)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        1.0, (255, 0, 0), lineType=30)\n",
    "        return frame\n",
    "    \n",
    "    def toggle_recording(self, speed):\n",
    "        \"\"\"Handle recording keypress, creates output stream and file\"\"\"\n",
    "        if speed == 0:\n",
    "            return\n",
    "        self.record = not self.record\n",
    "\n",
    "        if self.record:\n",
    "            datename = [\n",
    "                # os.getenv('HOME'), \n",
    "                datetime.datetime.now().strftime(self.date_fmt),\n",
    "            ]\n",
    "            self.out_name = 'C:/Users/eliza/OneDrive/Computer_Files/Desktop/Desktop/Fourth_Year/SPRING_2019/PROCESS_CONTROLS/Untitled_Folder/telloCV-master/Images/tello-{}.mp4'.format(*datename)\n",
    "            print(\"Outputting video to:\", self.out_name)\n",
    "\n",
    "\n",
    "            self.out_stream_writer = cv2.VideoWriter(self.out_name,cv2.CV_FOURCC('P','I','M','1'),25,(self.vid_stream.width,self.vid_stream.height))\n",
    "\n",
    "\n",
    "            # self.out_file = av.open(self.out_name, 'w')\n",
    "            # self.start_time = time.time()\n",
    "            # self.out_stream = self.out_file.add_stream(\n",
    "            #     'mpeg4', self.vid_stream.rate)\n",
    "            # self.out_stream.pix_fmt = 'yuv420p'\n",
    "            # self.out_stream.width = self.vid_stream.width\n",
    "            # self.out_stream.height = self.vid_stream.height\n",
    "\n",
    "        if not self.record:\n",
    "            print(\"Video saved to \", self.out_name)\n",
    "            # self.out_file.close()\n",
    "            # self.out_stream = None\n",
    "\n",
    "            self.out_stream_writer.release()\n",
    "            self.out_stream_writer = None\n",
    "\n",
    "    def record_vid(self, frame):\n",
    "        \"\"\"\n",
    "        convert frames to packets and write to file\n",
    "        \"\"\"\n",
    "        if not self.out_stream_writer is None:\n",
    "            print(frame.width, frame.height, frame.format.name)\n",
    "            arr = frame.to_ndarray(format='rgb24')\n",
    "            # print(arr)\n",
    "            # print(arr.shape)\n",
    "\n",
    "\n",
    "            self.out_stream_writer.write(arr)\n",
    "\n",
    "\n",
    "        # new_frame = av.video.frame.VideoFrame.from_ndarray(arr)\n",
    "        # # new_frame = av.VideoFrame(\n",
    "        # #     width=frame.width, height=frame.height, format=frame.format.name)\n",
    "        # # for i in range(len(frame.planes)):\n",
    "        # #     print(type(frame.planes[i]))\n",
    "        # #     #new_frame.planes[i].update(frame.planes[i])\n",
    "        # #     new_frame.planes[i].update(frame.planes[i].reformat(width=frame.width, height=frame.height, format=frame.format.name))\n",
    "        # pkt = None\n",
    "        # try:\n",
    "        #     # pkt = self.out_stream.encode(new_frame)\n",
    "        #     pkt = self.out_stream.encode(new_frame)\n",
    "        # except IOError as err:\n",
    "        #     print(\"encoding failed: {0}\".format(err))\n",
    "        # if pkt is not None:\n",
    "        #     try:\n",
    "        #         self.out_file.mux(pkt)\n",
    "        #     except IOError:\n",
    "        #         print('mux failed: ' + str(pkt))\n",
    "\n",
    "    def take_picture(self, speed):\n",
    "        \"\"\"Tell drone to take picture, image sent to file handler\"\"\"\n",
    "        if speed == 0:\n",
    "            return\n",
    "        self.drone.take_picture()\n",
    "\n",
    "    def palm_land(self, speed):\n",
    "        \"\"\"Tell drone to land\"\"\"\n",
    "        if speed == 0:\n",
    "            return\n",
    "        self.drone.palm_land()\n",
    "\n",
    "    def toggle_tracking(self, speed):\n",
    "        \"\"\" Handle tracking keypress\"\"\"\n",
    "        if speed == 0:  # handle key up event\n",
    "            return\n",
    "        self.tracking = not self.tracking\n",
    "        print(\"tracking:\", self.tracking)\n",
    "        return\n",
    "\n",
    "    def toggle_zoom(self, speed):\n",
    "        \"\"\"\n",
    "        In \"video\" mode the self.drone sends 1280x720 frames.\n",
    "        In \"photo\" mode it sends 2592x1936 (952x720) frames.\n",
    "        The video will always be centered in the window.\n",
    "        In photo mode, if we keep the window at 1280x720 that gives us ~160px on\n",
    "        each side for status information, which is ample.\n",
    "        Video mode is harder because then we need to abandon the 16:9 display size\n",
    "        if we want to put the HUD next to the video.\n",
    "        \"\"\"\n",
    "        if speed == 0:\n",
    "            return\n",
    "        self.drone.set_video_mode(not self.drone.zoom)\n",
    "\n",
    "    def flight_data_handler(self, event, sender, data):\n",
    "        \"\"\"Listener to flight data from the drone.\"\"\"\n",
    "        text = str(data)\n",
    "        if self.prev_flight_data != text:\n",
    "            self.prev_flight_data = text\n",
    "\n",
    "    def handle_flight_received(self, event, sender, data):\n",
    "        \"\"\"Create a file in ~/Pictures/ to receive image from the drone\"\"\"\n",
    "        path = 'C:Users/eliza/OneDrive/Computer_Files/Desktop/Desktop/Fourth_Year/SPRING_2019/PROCESS_CONTROLS/Untitled_Folder/telloCV-master/Images/tello-{}.jpeg' % (\n",
    "            os.getenv('HOME'),\n",
    "            datetime.datetime.now().strftime(self.date_fmt))\n",
    "        with open(path, 'wb') as out_file:\n",
    "            out_file.write(data)\n",
    "        print('Saved photo to' % path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOqiyYb7eMdv",
    "pycharm": {}
   },
   "source": [
    "### B.) Tracker.py\n",
    "This code is responsible for the image recognition and object tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B06ck8lDef7u",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A tracker class for controlling the Tello and some sample code for showing how\n",
    "it works. you can test it using your webcam or a video file to make sure it works.\n",
    "\n",
    "it computes a vector of the ball's direction from the center of the\n",
    "screen. The axes are shown below (assuming a frame width and height of 600x400):\n",
    "+y                 (0,200)\n",
    "\n",
    "\n",
    "Y  (-300, 0)        (0,0)               (300,0)\n",
    "\n",
    "\n",
    "-Y                 (0,-200)\n",
    "-X                    X                    +X\n",
    "\n",
    "Based on the tutorial:\n",
    "https://www.pyimagesearch.com/2015/09/14/ball-tracking-with-opencv/\n",
    "\n",
    "Usage:\n",
    "for existing video:\n",
    "python tracker.py --video ball_tracking_example.mp4\n",
    "For live feed:\n",
    "python tracking.py\n",
    "\n",
    "@author Leonie Buckley and Jonathan Byrne\n",
    "@copyright 2018 see license file for details\n",
    "\"\"\"\n",
    "\n",
    "# import the necessary packages\n",
    "import argparse\n",
    "import time\n",
    "import cv2\n",
    "import imutils\n",
    "from imutils.video import VideoStream\n",
    "\n",
    "def main():\n",
    "\t\"\"\"Handles inpur from file or stream, tests the tracker class\"\"\"\n",
    "\targ_parse = argparse.ArgumentParser()\n",
    "\targ_parse.add_argument(\"-v\", \"--video\",\n",
    "\t\t\t\t\t\t   help=\"path to the (optional) video file\")\n",
    "\targs = vars(arg_parse.parse_args())\n",
    "\n",
    "\t# define the lower and upper boundaries of the \"green\"\n",
    "\t# ball in the HSV color space. NB the hue range in\n",
    "\t# opencv is 180, normally it is 360\n",
    "\t#green_lower = (50, 50, 50)\n",
    "\t#green_upper = (70, 255, 255)\n",
    "\t# red_lower = (0, 50, 50)\n",
    "\t# red_upper = (20, 255, 255)\n",
    "\t# blue_lower = (110, 50, 50)\n",
    "\t# upper_blue = (130, 255, 255)\n",
    "\tcolor_lower = (110, 50, 50)  #BGR\n",
    "\tcolor_upper = (130, 255, 255) #RGB\n",
    "\n",
    "\n",
    "\t# if a video path was not supplied, grab the reference\n",
    "\t# to the webcam\n",
    "\tif not args.get(\"video\", False):\n",
    "\t\tvid_stream = VideoStream(src=0).start()\n",
    "\n",
    "\t# otherwise, grab a reference to the video file\n",
    "\telse:\n",
    "\t\tvid_stream = cv2.VideoCapture(args[\"video\"])\n",
    "\n",
    "\t# allow the camera or video file to warm up\n",
    "\ttime.sleep(2.0)\n",
    "\tstream = args.get(\"video\", False)\n",
    "\tframe = get_frame(vid_stream, stream)\n",
    "\theight, width = frame.shape[0], frame.shape[1]\n",
    "\tcolortracker = Tracker(height, width, color_lower, color_upper)\n",
    "\t\n",
    "\n",
    "\t# keep looping until no more frames\n",
    "\tmore_frames = True\n",
    "\twhile more_frames:\n",
    "\t\tcolortracker.track(frame)\n",
    "\t\tframe = colortracker.draw_arrows(frame)\n",
    "\t\tshow(frame)\n",
    "\t\tframe = get_frame(vid_stream, stream)\n",
    "\t\tif frame is None:\n",
    "\t\t\tmore_frames = False\n",
    "\n",
    "\t# if we are not using a video file, stop the camera video stream\n",
    "\tif not args.get(\"video\", False):\n",
    "\t\tvid_stream.stop()\n",
    "\n",
    "\t# otherwise, release the camera\n",
    "\telse:\n",
    "\t\tvid_stream.release()\n",
    "\n",
    "\t# close all windows\n",
    "\tcv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def get_frame(vid_stream, stream):\n",
    "\t\"\"\"grab the current video frame\"\"\"\n",
    "\tframe = vid_stream.read()\n",
    "\t# handle the frame from VideoCapture or VideoStream\n",
    "\tframe = frame[1] if stream else frame\n",
    "\t# if we are viewing a video and we did not grab a frame,\n",
    "\t# then we have reached the end of the video\n",
    "\tif frame is None:\n",
    "\t\treturn None\n",
    "\telse:\n",
    "\t\tframe = imutils.resize(frame, width=600)\n",
    "\t\treturn frame\n",
    "\n",
    "\n",
    "def show(frame):\n",
    "\t\"\"\"show the frame to cv2 window\"\"\"\n",
    "\tcv2.imshow(\"Frame\", frame)\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "\t# if the 'q' key is pressed, stop the loop\n",
    "\tif key == ord(\"q\"):\n",
    "\t\texit()\n",
    "\n",
    "\n",
    "class Tracker:\n",
    "\t\"\"\"\n",
    "\tA basic color tracker, it will look for colors in a range and\n",
    "\tcreate an x and y offset valuefrom the midpoint\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, height, width, color_lower, color_upper):\n",
    "\t\tself.color_lower = color_lower\n",
    "\t\tself.color_upper = color_upper\n",
    "\t\tself.midx = int(width / 2)\n",
    "\t\tself.midy = int(height / 2)\n",
    "\t\tself.xoffset = 0\n",
    "\t\tself.yoffset = 0\n",
    "\n",
    "\tdef draw_arrows(self, frame):\n",
    "\t\t\"\"\"Show the direction vector output in the cv2 window\"\"\"\n",
    "\t\t#cv2.putText(frame,\"Color:\", (0, 35), cv2.FONT_HERSHEY_SIMPLEX, 1, 255, thickness=2)\n",
    "\t\tcv2.arrowedLine(frame, (self.midx, self.midy),\n",
    "\t\t\t\t\t\t(self.midx + self.xoffset, self.midy - self.yoffset),\n",
    "\t\t\t\t\t\t(0, 0, 255), 5)\n",
    "\t\treturn frame\n",
    "\n",
    "\tdef track(self, frame):\n",
    "\t\t\"\"\"Simple HSV color space tracking\"\"\"\n",
    "\t\t# resize the frame, blur it, and convert it to the HSV\n",
    "\t\t# color space\n",
    "\t\tblurred = cv2.GaussianBlur(frame, (11, 11), 0)\n",
    "\t\thsv = cv2.cvtColor(blurred, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "\t\t# construct a mask for the color then perform\n",
    "\t\t# a series of dilations and erosions to remove any small\n",
    "\t\t# blobs left in the mask\n",
    "\t\tmask = cv2.inRange(hsv, self.color_lower, self.color_upper)\n",
    "\t\tmask = cv2.erode(mask, None, iterations=2)\n",
    "\t\tmask = cv2.dilate(mask, None, iterations=2)\n",
    "\n",
    "\t\t# find contours in the mask and initialize the current\n",
    "\t\t# (x, y) center of the ball\n",
    "\t\tcnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL,\n",
    "\t\t\t\t\t\t\t\tcv2.CHAIN_APPROX_SIMPLE)\n",
    "\t\tcnts = cnts[0]\n",
    "\t\tcenter = None\n",
    "\n",
    "\t\t# only proceed if at least one contour was found\n",
    "\t\tif len(cnts) > 0:\n",
    "\t\t\t# find the largest contour in the mask, then use\n",
    "\t\t\t# it to compute the minimum enclosing circle and\n",
    "\t\t\t# centroid\n",
    "\n",
    "\t\t\t#initialize temp variables\n",
    "\t\t\tlowest_error = None \n",
    "\t\t\tbest_xoffset = 0\n",
    "\t\t\tbest_yoffset = 0\n",
    "\t\t\tbest_x = 0\n",
    "\t\t\tbest_y = 0\n",
    "\t\t\tbest_radius = 0\n",
    "\t\t\tbest_center = (0,0)\n",
    "\n",
    "\t\t\tfor c in cnts: #iterate through every contour\n",
    "\t\t\t\t#c = max(cnts, key=cv2.contourArea)\n",
    "\t\t\t\t((x, y), radius) = cv2.minEnclosingCircle(c)\n",
    "\t\t\t\tM = cv2.moments(c)\n",
    "\t\t\t\tcenter = (int(M[\"m10\"] / M[\"m00\"]), int(M[\"m01\"] / M[\"m00\"]))\n",
    "\n",
    "\t\t\t\t# only proceed if the radius meets a minimum size\n",
    "\t\t\t\tif radius > 10:\n",
    "\t\t\t\t\t\n",
    "\n",
    "\t\t\t\t\ttemp_xoffset = int(center[0] - self.midx) #store the xoffset and yoffset for each iteration of the loop\n",
    "\t\t\t\t\ttemp_yoffset = int(self.midy - center[1])\n",
    "\n",
    "\t\t\t\t\t#calculate the distance from the previous x and y by finding the squared error\n",
    "\t\t\t\t\tsqrd_error = ((temp_xoffset-self.xoffset)**2) + ((temp_yoffset - self.yoffset)**2)\n",
    "\n",
    "\t\t\t\t\t#Set xoffset and yoffset for the lowest possible distance\n",
    "\t\t\t\t\tif lowest_error is None or sqrd_error < lowest_error:\n",
    "\t\t\t\t\t\tlowest_error = sqrd_error\n",
    "\t\t\t\t\t\tbest_xoffset = temp_xoffset\n",
    "\t\t\t\t\t\tbest_yoffset = temp_yoffset\n",
    "\t\t\t\t\t\tbest_x = x\n",
    "\t\t\t\t\t\tbest_y = y\n",
    "\t\t\t\t\t\tbest_radius = radius\n",
    "\t\t\t\t\t\tbest_center = center\n",
    "\n",
    "\n",
    "\t\t\t#Set xoffset and yoffset to the best values calculated in the for loop\t\t\n",
    "\t\t\tself.xoffset = best_xoffset\n",
    "\t\t\tself.yoffset = best_yoffset\n",
    "\t\t\t# draw the circle and centroid on the frame,\n",
    "\t\t\t# then update the list of tracked points\n",
    "\t\t\tcv2.circle(frame, (int(best_x), int(best_y)), int(best_radius), # Draws the yellow circle on video stream\n",
    "\t\t\t\t\t(0, 255, 255), 2)\n",
    "\t\t\tcv2.circle(frame, best_center, 5, (0, 0, 255), -1) # Draws a red dot in the center of the yellow circle\n",
    "\t\telse:\n",
    "\t\t\tself.xoffset = 0\n",
    "\t\t\tself.yoffset = 0\n",
    "\n",
    "\t\treturn self.xoffset, self.yoffset #feed the optimized xoffset and yoffset to telloCV.py\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "<!--NAVIGATION-->\n",
    "< [Diabetes: Controlling Blood Glucose Concentrations](http://nbviewer.jupyter.org/github/jckantor/CBE30338/blob/master/notebooks/C.01-Diabetes-Controlling-Blood-Glucose-Concentrations.ipynb) | [Contents](toc.ipynb) |<p><a href=\"https://colab.research.google.com/github/jckantor/CBE30338/blob/master/notebooks/C.02-Visual-Tracking-of-an-Object-with-a-Drone.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open in Google Colaboratory\"></a>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C.02-Object-Tracking-with-a-Drone.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
